{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import fbeta_score\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "from time import ctime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    im = cv2.imread(path)\n",
    "    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im - imagenet_stats[0]) / imagenet_stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data partitions\n",
    "with open(os.path.join(DATA_DIR, 'partition.p'), 'rb') as f:\n",
    "    partition = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(im, r, c, target_r, target_c):\n",
    "    return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "# random crop to the original size\n",
    "def random_crop(x, r_pix=8):\n",
    "    \"\"\"Returns a random crop\"\"\"\n",
    "    r, c, *_ = x.shape\n",
    "    r, c, *_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(2*rand_r*r_pix).astype(int)\n",
    "    start_c = np.floor(2*rand_c*c_pix).astype(int)\n",
    "    return crop(x, start_r, start_c, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "def center_crop(x, r_pix=8):\n",
    "    r, c, *_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    return crop(x, r_pix, c_pix, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "\n",
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    \"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r, c, *_ = im.shape\n",
    "    M = cv2.getRotationMatrix2D((c/2, r/2), deg, 1)\n",
    "    return cv2.warpAffine(im, M, (c,r), borderMode=mode, \n",
    "                          flags=cv2.WARP_FILL_OUTLIERS+interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetDataset(Dataset):\n",
    "    def __init__(self, img_folder, list_IDs, csv_path, transforms=False):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_folder = img_folder\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.y = self.mlb.fit_transform([tag.split() for tag in self.df.tags])\n",
    "        self.y = dict(zip(self.df['image_name'], self.y))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.list_IDs[idx]\n",
    "        img_path = os.path.join(self.img_folder, name + '.jpg')\n",
    "        x = cv2.imread(img_path).astype(np.float32)\n",
    "        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB) / 255\n",
    "        if self.transforms:\n",
    "            rdeg = (np.random.random() - 0.50) * 20\n",
    "            x = rotate_cv(x, rdeg)\n",
    "            x = random_crop(x)\n",
    "            if np.random.random() > 0.5:\n",
    "                x = np.fliplr(x).copy()\n",
    "        else:\n",
    "            x = center_crop(x)\n",
    "        x = normalize(x)\n",
    "        return np.rollaxis(x, 2), self.y[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, factor, init_lr=0.01, lr_decay_epoch=7):\n",
    "    '''\n",
    "    Decay learning rate by a factor every lr_decay_epoch epochs.\n",
    "    '''\n",
    "    lr = init_lr * (factor**(epoch // lr_decay_epoch))\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print(\"Setting base LR to %.8f\" % lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer, lr0, diff_lr_factors):\n",
    "    '''\n",
    "    Creates an optimizer for a NN segmented into groups, with a differential\n",
    "    learning rate across groups according to a multiplicative factor for each\n",
    "    group given by group_lrs\n",
    "    '''\n",
    "    n_groups = len(diff_lr_factors)\n",
    "    param_groups = [list(model.groups[i].parameters()) for i in range(n_groups)]\n",
    "    params = [{'params': p, 'lr': lr0/diff_factor} for p, diff_factor in zip(param_groups, diff_lr_factors)]\n",
    "    return optimizer(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_dl, threshold):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    true_labels, predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm_notebook(val_dl, desc='Validation metrics',\n",
    "                                 total=len(val_dl)):\n",
    "            true_labels.append(target.cpu().numpy())\n",
    "            data, target = data.cuda().float(), target.cuda().float()\n",
    "\n",
    "            pred = model(data)\n",
    "            predictions.append(F.sigmoid(pred).cpu().numpy())\n",
    "            total_loss += F.binary_cross_entropy_with_logits(pred,\n",
    "                                                             target).item()\n",
    "\n",
    "        avg_loss = total_loss / len(val_dl)\n",
    "        predictions = np.vstack(predictions)\n",
    "        true_labels = np.vstack(true_labels)\n",
    "        f2_score = fbeta_score(true_labels, predictions > threshold,\n",
    "                               beta=2, average='samples')\n",
    "        return f2_score, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/ubuntu/.cache/torch/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 553433881/553433881 [00:10<00:00, 54869693.15it/s]\n"
     ]
    }
   ],
   "source": [
    "vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.vgg = models.vgg19(pretrained=True)\n",
    "        # freezing parameters\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        # modify the final linear layer\n",
    "        num_features = self.vgg.classifier[6].in_features\n",
    "        self.vgg.classifier = self.vgg.classifier[:6]\n",
    "        # separate layers into two groups\n",
    "        layers = list(self.vgg.children())\n",
    "        self.groups = nn.ModuleList([nn.Sequential(*h) for h in [layers[:2], layers[2:]]])\n",
    "        self.groups.append(nn.Linear(num_features, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.groups[0](x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for group in self.groups[1:]:\n",
    "            x = group(x)\n",
    "        return x\n",
    "\n",
    "    def unfreeze(self,  group_idx: int):\n",
    "        group = self.groups[group_idx]\n",
    "        parameters = filter(lambda x: hasattr(x, 'requires_grad'),\n",
    "                            group.parameters())\n",
    "        for p in parameters:\n",
    "            p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = VGG19(num_classes=17).cuda()\n",
    "\n",
    "# datasets\n",
    "train_ds = PlanetDataset(os.path.join(DATA_DIR, 'train-jpg'), \n",
    "                        partition['inner_train'],\n",
    "                        os.path.join(DATA_DIR, 'train_v2.csv'),\n",
    "                        True)\n",
    "\n",
    "val_ds = PlanetDataset(os.path.join(DATA_DIR, 'train-jpg'),\n",
    "                    partition['validation'],\n",
    "                    os.path.join(DATA_DIR, 'train_v2.csv'))\n",
    "\n",
    "# data loaders\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True,\n",
    "                    shuffle=True)\n",
    "\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data flow through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = iter(train_dl).next()\n",
    "data = data.cuda().float()\n",
    "label = label.cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.vgg.features(data)\n",
    "avgpool = model.vgg.avgpool(features)\n",
    "avgpool = avgpool.view(avgpool.size(0), -1)\n",
    "classifier = model.vgg.classifier(avgpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing training loop with constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  3 01:00:01 2019\n",
      "Epoch 1 (Batch 0 / 506)\t Train loss: 0.188\n",
      "Epoch 1 (Batch 100 / 506)\t Train loss: 0.245\n",
      "Epoch 1 (Batch 200 / 506)\t Train loss: 0.217\n",
      "Epoch 1 (Batch 300 / 506)\t Train loss: 0.149\n",
      "Epoch 1 (Batch 400 / 506)\t Train loss: 0.196\n",
      "Epoch 1 (Batch 500 / 506)\t Train loss: 0.200\n",
      "Epoch 1\t Train loss: 0.210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c67867ab4741d3b2328433f9ea7a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation metrics', max=127, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 \t Validation loss: 0.175, F2 score: 0.859\n",
      "Sat Aug  3 01:06:19 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (Batch 0 / 506)\t Train loss: 0.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7410cc4f03be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(ctime())\n",
    "    iterations = epochs*len(train_dl)\n",
    "    idx = 0\n",
    "    total_loss = 0\n",
    "    # training loop\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        data, target = data.cuda().float(), target.cuda().float()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        idx += 1\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Epoch %d (Batch %d / %d)\\t Train loss: %.3f\" % \\\n",
    "                (epoch+1, batch_idx, len(train_dl), loss.item()))\n",
    "    # train loss\n",
    "    train_loss = total_loss / len(train_dl)\n",
    "    print(\"Epoch %d\\t Train loss: %.3f\" % (epoch+1, train_loss))\n",
    "    val_f2_score, val_loss = validate(model, val_dl, 0.2)\n",
    "    print(\"Epoch %d \\t Validation loss: %.3f, F2 score: %.3f\" % (epoch+1, val_loss, val_f2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing training loop with differential and decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting base LR to 0.01000000\n",
      "Epoch 1 (Batch 0 / 506)\t Train loss: 0.704\n",
      "Epoch 1 (Batch 100 / 506)\t Train loss: 0.176\n",
      "Epoch 1 (Batch 200 / 506)\t Train loss: 0.235\n",
      "Epoch 1 (Batch 300 / 506)\t Train loss: 0.192\n",
      "Epoch 1 (Batch 400 / 506)\t Train loss: 0.167\n",
      "Epoch 1 (Batch 500 / 506)\t Train loss: 0.249\n",
      "Epoch 1\t Train loss: 0.192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa515b50e4fb4619b639d7ba3e0c1635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation metrics', max=127, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 \t Validation loss: 0.159, F2 score: 0.864\n",
      "Epoch 2 (Batch 0 / 506)\t Train loss: 0.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/planet-imagery/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9a35c2beb6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model\n",
    "MODEL = VGG19(num_classes=17).cuda()\n",
    "\n",
    "# datasets\n",
    "train_ds = PlanetDataset(os.path.join(DATA_DIR, 'train-jpg'), \n",
    "                        partition['inner_train'],\n",
    "                        os.path.join(DATA_DIR, 'train_v2.csv'),\n",
    "                        True)\n",
    "\n",
    "val_ds = PlanetDataset(os.path.join(DATA_DIR, 'train-jpg'),\n",
    "                    partition['validation'],\n",
    "                    os.path.join(DATA_DIR, 'train_v2.csv'))\n",
    "\n",
    "# data loaders\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True,\n",
    "                    shuffle=True)\n",
    "\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True)\n",
    "\n",
    "# optimizer\n",
    "BASE_OPTIMIZER = optim.Adam\n",
    "DIFF_LR_FACTORS = [9, 3, 1]\n",
    "INIT_LR_0 = 0.01\n",
    "EPOCHS = 40\n",
    "\n",
    "best_score = 0.0\n",
    "# create optimizer with differential learning rates\n",
    "optimizer = create_optimizer(model, BASE_OPTIMIZER, INIT_LR_0, DIFF_LR_FACTORS)\n",
    "iterations = EPOCHS*len(train_dl)\n",
    "idx = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    lr0 = lr_scheduler(epoch, 0.1, INIT_LR_0, 5)  # set base lr for this epoch\n",
    "    optimizer = create_optimizer(model, BASE_OPTIMIZER, lr0, DIFF_LR_FACTORS)\n",
    "    total_loss = 0\n",
    "    # training loop\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        data, target = data.cuda().float(), target.cuda().float()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        idx += 1\n",
    "        # unfreeze deeper layers sequentially\n",
    "        if idx == int(0.1*iterations):\n",
    "            model.unfreeze(1)\n",
    "            print(\"Iteration %d: Unfreezing group 1\" % idx)\n",
    "        if idx == int(0.2*iterations):\n",
    "            model.unfreeze(0)\n",
    "            print(\"Iteration %d: Unfreezing group 0\" % idx)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Epoch %d (Batch %d / %d)\\t Train loss: %.3f\" % \\\n",
    "                (epoch+1, batch_idx, len(train_dl), loss.item()))\n",
    "    # train loss\n",
    "    train_loss = total_loss / len(train_dl)\n",
    "    print(\"Epoch %d\\t Train loss: %.3f\" % (epoch+1, train_loss))\n",
    "    # validation scores\n",
    "    val_f2_score, val_loss = validate(model, val_dl, 0.2)\n",
    "    print(\"Epoch %d \\t Validation loss: %.3f, F2 score: %.3f\" % \\\n",
    "        (epoch+1, val_loss, val_f2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_planet-imagery)",
   "language": "python",
   "name": "conda_planet-imagery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
